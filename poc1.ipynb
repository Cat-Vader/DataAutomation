{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Building a data analyst agent with LangGraph and Azure Container Apps dynamic sessions\n",
    "\n",
    "In this example we'll build an agent that can query a Postgres database and run Python code to analyze the retrieved data. We'll use [LangGraph](https://langchain-ai.github.io/langgraph/) for agent orchestration and [Azure Container Apps dynamic sessions](https://python.langchain.com/v0.2/docs/integrations/tools/azure_dynamic_sessions/) for safe Python code execution.\n",
    "\n",
    "**NOTE**: Building LLM systems that interact with SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, see our [security guidelines](https://python.langchain.com/v0.2/docs/security/).\n",
    "## Setup\n",
    "\n",
    "Let's get set up by installing our Python dependencies and setting our OpenAI credentials, Azure Container Apps sessions pool endpoint, and our SQL database connection string.\n",
    "\n",
    "### Install dependencies\n",
    "%pip install -qU langgraph langchain-azure-dynamic-sessions langchain-openai langchain-community pandas matplotlib\n",
    "### Set credentials\n",
    "\n",
    "By default this demo uses:\n",
    "- Azure OpenAI for the model: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource\n",
    "- Azure PostgreSQL for the db: https://learn.microsoft.com/en-us/cli/azure/postgres/server?view=azure-cli-latest#az-postgres-server-create\n",
    "- Azure Container Apps dynamic sessions for code execution: https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter?\n",
    "\n",
    "This LangGraph architecture can also be used with any other [tool-calling LLM](https://python.langchain.com/v0.2/docs/how_to/tool_calling) and any SQL database.\n",
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Use os.getenv() to get the values, with a prompt as fallback\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") or input(\"Azure OpenAI API key: \")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or input(\"Azure OpenAI endpoint: \")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") or input(\"Azure OpenAI deployment name: \")\n",
    "SESSIONS_POOL_MANAGEMENT_ENDPOINT = os.getenv(\"SESSIONS_POOL_MANAGEMENT_ENDPOINT\") or input(\"Azure Container Apps dynamic sessions pool management endpoint: \")\n",
    "SQL_DB_CONNECTION_STRING = os.getenv(\"SQL_DB_CONNECTION_STRING\") or input(\"PostgreSQL connection string: \")\n",
    "\n",
    "# Set environment variables if they weren't in the .env file\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "# Print a message to confirm the variables are set (optional)\n",
    "print(\"Environment variables loaded successfully.\")\n",
    "### Imports\n",
    "import ast\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import operator\n",
    "from functools import partial\n",
    "from typing import Annotated, List, Literal, Optional, Sequence, TypedDict\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from langchain_azure_dynamic_sessions import SessionsPythonREPLTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "%pip install psycopg2-binary\n",
    "## Instantiate model, DB, code interpreter\n",
    "\n",
    "We'll use the LangChain [SQLDatabase](https://python.langchain.com/v0.2/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase) interface to connect to our DB and query it. This works with any SQL database supported by [SQLAlchemy](https://www.sqlalchemy.org/).\n",
    "db = SQLDatabase.from_uri(SQL_DB_CONNECTION_STRING)\n",
    "def verify_database_content():\n",
    "    session_count = db.run(\"SELECT COUNT(*) FROM sessions\", fetch=\"one\")\n",
    "    message_count = db.run(\"SELECT COUNT(*) FROM messages\", fetch=\"one\")\n",
    "    print(f\"Total sessions in database: {session_count[0]}\")\n",
    "    print(f\"Total messages in database: {message_count[0]}\")\n",
    "    print(\"Sample of sessions:\")\n",
    "    print(db.run(\"SELECT * FROM sessions LIMIT 5\", fetch=\"all\"))\n",
    "    print(\"Sample of messages:\")\n",
    "    print(db.run(\"SELECT * FROM messages LIMIT 5\", fetch=\"all\"))\n",
    "\n",
    "# Call this function before running any queries\n",
    "verify_database_content()\n",
    "\n",
    "For our LLM we need to make sure that we use a model that supports [tool-calling](https://python.langchain.com/v0.2/docs/how_to/tool_calling).\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=AZURE_OPENAI_DEPLOYMENT_NAME, openai_api_version=\"2024-02-01\"\n",
    ")\n",
    "And the [dynamic sessions tool](https://python.langchain.com/v0.2/docs/integrations/tools/azure_container_apps_dynamic_sessions/) is what we'll use for code execution.\n",
    "repl = SessionsPythonREPLTool(\n",
    "    pool_management_endpoint=SESSIONS_POOL_MANAGEMENT_ENDPOINT\n",
    ")\n",
    "## Define graph\n",
    "\n",
    "Now we're ready to define our application logic. The core elements are the [agent State, Nodes, and Edges](https://langchain-ai.github.io/langgraph/concepts/#core-design).\n",
    "\n",
    "### Define State\n",
    "We'll use a simple agent State which is just a list of messages that every Node can append to:\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "Since our code interpreter can return results like base64-encoded images which we don't want to pass back to the model, we'll create a custom Tool message that allows us to track raw Tool outputs without sending them back to the model.\n",
    "class RawToolMessage(ToolMessage):\n",
    "    \"\"\"\n",
    "    Customized Tool message that lets us pass around the raw tool outputs (along with string contents for passing back to the model).\n",
    "    \"\"\"\n",
    "\n",
    "    raw: dict\n",
    "    \"\"\"Arbitrary (non-string) tool outputs. Won't be sent to model.\"\"\"\n",
    "    tool_name: str\n",
    "    \"\"\"Name of tool that generated output.\"\"\"\n",
    "### Define Nodes\n",
    "First we'll define a node for calling our model. We need to make sure to bind our tools to the model so that it knows to call them. We'll also specify in our prompt the schema of the SQL tables the model has access to, so that it can write relevant SQL queries.\n",
    "We'll use our models tool-calling abilities to reliably generate our SQL queries and Python code. To do this we need to define schemas for our tools that the model can use for structuring its tool calls.\n",
    "\n",
    "Note that the class names, docstrings, and attribute typing and descriptions are crucial here, as they're actually passed in to the model (you can effectively think of them as part of the prompt).\n",
    "# Tool schema for querying SQL db\n",
    "class create_df_from_sql(BaseModel):\n",
    "    \"\"\"Execute a PostgreSQL SELECT statement and use the results to create a DataFrame with the given column names.\"\"\"\n",
    "\n",
    "    select_query: str = Field(..., description=\"A PostgreSQL SELECT statement. Must return at least one row of data.\")\n",
    "    df_columns: List[str] = Field(\n",
    "        ..., description=\"Ordered names to give the DataFrame columns. Must match the number and order of columns in the SELECT statement.\"\n",
    "    )\n",
    "    df_name: str = Field(\n",
    "        ..., description=\"The name to give the DataFrame variable in downstream code.\"\n",
    "    )\n",
    "\n",
    "# Tool schema for writing Python code\n",
    "class python_shell(BaseModel):\n",
    "    \"\"\"Execute Python code that analyzes the DataFrames that have been generated. Make sure to print any important results.\"\"\"\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The code to execute. Make sure to print any important results.\",\n",
    "    )\n",
    "system_prompt = f\"\"\"\\\n",
    "You are an expert at PostgreSQL and Python, specializing in chat data analysis. You have access to a PostgreSQL database with the following tables:\n",
    "\n",
    "{db.table_info}\n",
    "\n",
    "Given a user question related to the chat data in the database, follow these steps:\n",
    "1. Formulate an appropriate SQL query to retrieve relevant data from the tables.\n",
    "   - Always use explicit column names in your SELECT statements rather than using SELECT *.\n",
    "   - Ensure that your query returns at least one row of data.\n",
    "   - If you're counting or aggregating data, use appropriate column aliases.\n",
    "2. Use the create_df_from_sql tool to execute the SQL query and create a DataFrame. \n",
    "   - Specify the exact column names that your SQL query will return in the df_columns argument.\n",
    "   - Ensure that the number of columns specified matches the number of columns in your SQL query.\n",
    "3. Utilize the python_shell to perform any necessary analysis on the DataFrame to answer the user's question.\n",
    "4. Provide clear, concise answers with relevant statistics or visualizations when appropriate.\n",
    "\n",
    "Remember to join the 'sessions' and 'messages' tables when needed to get comprehensive information about the chats.\n",
    "\n",
    "Common analysis tasks might include:\n",
    "- Message frequency analysis\n",
    "- User engagement metrics\n",
    "- Content analysis of messages\n",
    "- Time-based patterns in chat activity\n",
    "- Source or platform comparisons\n",
    "\n",
    "Always consider data privacy and avoid exposing any sensitive information in your responses.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def call_model(state: AgentState) -> dict:\n",
    "    \"\"\"Call model with tools passed in.\"\"\"\n",
    "    messages = []\n",
    "\n",
    "    chain = prompt | llm.bind_tools([create_df_from_sql, python_shell])\n",
    "    messages.append(chain.invoke({\"messages\": state[\"messages\"]}))\n",
    "\n",
    "    return {\"messages\": messages}\n",
    "Now we can define the node for executing any SQL queries that were generated by the model. Notice that after we run the query we convert the results into Pandas DataFrames — these will be uploaded the the code interpreter tool in the next step so that it can use the retrieved data.\n",
    "def execute_sql_query(state: AgentState) -> dict:\n",
    "    messages = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        if tool_call[\"name\"] != \"create_df_from_sql\":\n",
    "            continue\n",
    "        \n",
    "        query = tool_call[\"args\"][\"select_query\"]\n",
    "        print(f\"Executing SQL Query: {query}\")  # Debug print\n",
    "        \n",
    "        res = db.run(query, fetch=\"all\")\n",
    "        print(f\"Query result type: {type(res)}\")\n",
    "        print(f\"Query result length: {len(res)}\")\n",
    "        print(f\"Query result sample: {res[:5]}\")  # Debug print (first 5 rows)\n",
    "        \n",
    "        df_columns = tool_call[\"args\"][\"df_columns\"]\n",
    "        print(f\"Specified columns: {df_columns}\")\n",
    "        \n",
    "        if not res:\n",
    "            error_message = \"The SQL query returned no results.\"\n",
    "            print(error_message)\n",
    "            messages.append(RawToolMessage(error_message, raw={\"error\": error_message}, tool_call_id=tool_call[\"id\"], tool_name=tool_call[\"name\"]))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(res, columns=df_columns)\n",
    "            df_name = tool_call[\"args\"][\"df_name\"]\n",
    "            \n",
    "            print(f\"Created DataFrame {df_name} with shape: {df.shape}\")  # Debug print\n",
    "            print(f\"Sample of {df_name}:\\n{df.head()}\")  # Debug print\n",
    "            \n",
    "            messages.append(\n",
    "                RawToolMessage(\n",
    "                    f\"Generated dataframe {df_name} with columns {df_columns}. Shape: {df.shape}\",\n",
    "                    raw={df_name: df},\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                    tool_name=tool_call[\"name\"],\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error creating DataFrame: {str(e)}\"\n",
    "            print(error_message)\n",
    "            print(f\"res: {res}\")\n",
    "            print(f\"df_columns: {df_columns}\")\n",
    "            messages.append(RawToolMessage(error_message, raw={\"error\": error_message}, tool_call_id=tool_call[\"id\"], tool_name=tool_call[\"name\"]))\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "Now we need a node for executing any model-generated Python code. The key steps here are:\n",
    "- Uploading queried data to the code intepreter\n",
    "- Executing model generated code\n",
    "- Parsing results so that images are displayed and not passed in to future model calls\n",
    "\n",
    "To upload the queried data to the model we can take our DataFrames we generated by executing the SQL queries and upload them as CSVs to our code intepreter.\n",
    "def _upload_dfs_to_repl(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Upload generated dfs to code interpreter and return code for loading them.\n",
    "    \"\"\"\n",
    "    df_dicts = [\n",
    "        msg.raw\n",
    "        for msg in state[\"messages\"]\n",
    "        if isinstance(msg, RawToolMessage) and msg.tool_name == \"create_df_from_sql\"\n",
    "    ]\n",
    "    name_df_map = {name: df for df_dict in df_dicts for name, df in df_dict.items()}\n",
    "    \n",
    "    print(\"DataFrame map:\", name_df_map)  # Debug print\n",
    "    \n",
    "    # Data should be uploaded as a BinaryIO.\n",
    "    # Files will be uploaded to the \"/mnt/data/\" directory on the container.\n",
    "    for name, df in name_df_map.items():\n",
    "        print(f\"Processing DataFrame '{name}':\")  # Debug print\n",
    "        print(f\"Type: {type(df)}\")  # Debug print\n",
    "        print(f\"Content: {df}\")  # Debug print\n",
    "        \n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            buffer = io.StringIO()\n",
    "            df.to_csv(buffer, index=False)\n",
    "            buffer.seek(0)\n",
    "            repl.upload_file(data=buffer, remote_file_path=name + \".csv\")\n",
    "        elif isinstance(df, str):\n",
    "            # If it's a string, we'll assume it's already in CSV format\n",
    "            buffer = io.StringIO(df)\n",
    "            repl.upload_file(data=buffer, remote_file_path=name + \".csv\")\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected type for DataFrame '{name}': {type(df)}\")\n",
    "            continue\n",
    "\n",
    "    # Code for loading the uploaded files.\n",
    "    df_code = \"import pandas as pd\\n\" + \"\\n\".join(\n",
    "        f\"{name} = pd.read_csv('/mnt/data/{name}.csv')\" for name in name_df_map\n",
    "    )\n",
    "    return df_code\n",
    "\n",
    "\n",
    "def _repl_result_to_msg_content(repl_result: dict) -> str:\n",
    "    \"\"\"\n",
    "    Display images with including them in tool message content.\n",
    "    \"\"\"\n",
    "    content = {}\n",
    "    for k, v in repl_result.items():\n",
    "        # Any image results are returned as a dict of the form:\n",
    "        # {\"type\": \"image\", \"base64_data\": \"...\"}\n",
    "        if isinstance(repl_result[k], dict) and repl_result[k][\"type\"] == \"image\":\n",
    "            # Decode and display image\n",
    "            base64_str = repl_result[k][\"base64_data\"]\n",
    "            img = Image.open(io.BytesIO(base64.decodebytes(bytes(base64_str, \"utf-8\"))))\n",
    "            display(img)\n",
    "        else:\n",
    "            content[k] = repl_result[k]\n",
    "    return json.dumps(content, indent=2)\n",
    "\n",
    "def execute_python(state: AgentState) -> dict:\n",
    "    messages = []\n",
    "    \n",
    "    df_code = _upload_dfs_to_repl(state)\n",
    "    print(\"DataFrames uploaded to REPL:\")  # Debug print\n",
    "    print(df_code)  # Debug print\n",
    "    \n",
    "    last_ai_msg = [msg for msg in state[\"messages\"] if isinstance(msg, AIMessage)][-1]\n",
    "    for tool_call in last_ai_msg.tool_calls:\n",
    "        if tool_call[\"name\"] != \"python_shell\":\n",
    "            continue\n",
    "        \n",
    "        generated_code = tool_call[\"args\"][\"code\"]\n",
    "        print(\"Executing Python Code:\")  # Debug print\n",
    "        print(generated_code)  # Debug print\n",
    "        \n",
    "        try:\n",
    "            repl_result = repl.execute(df_code + \"\\n\" + generated_code)\n",
    "            print(\"Python Execution Result:\")  # Debug print\n",
    "            print(repl_result)  # Debug print\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error during Python execution: {str(e)}\"\n",
    "            print(error_message)  # Debug print\n",
    "            repl_result = {\"error\": error_message}\n",
    "        \n",
    "        messages.append(\n",
    "            RawToolMessage(\n",
    "                _repl_result_to_msg_content(repl_result),\n",
    "                raw=repl_result,\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "                tool_name=tool_call[\"name\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": messages}\n",
    "### Define Edges\n",
    "\n",
    "Now we're ready to put all the pieces together into a graph.\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    If any Tool messages were generated in the last cycle that means we need to call the model again to interpret the latest results.\n",
    "    \"\"\"\n",
    "    return \"execute_sql_query\" if state[\"messages\"][-1].tool_calls else END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"execute_sql_query\", execute_sql_query)\n",
    "workflow.add_node(\"execute_python\", execute_python)\n",
    "\n",
    "workflow.set_entry_point(\"call_model\")\n",
    "workflow.add_edge(\"execute_sql_query\", \"execute_python\")\n",
    "workflow.add_edge(\"execute_python\", \"call_model\")\n",
    "workflow.add_conditional_edges(\"call_model\", should_continue)\n",
    "\n",
    "app = workflow.compile()\n",
    "%pip install grandalf\n",
    "print(app.get_graph().draw_ascii())\n",
    "## Test it out\n",
    "\n",
    "Replace these examples with questions related to the database you've connected your agent to.\n",
    "output = app.invoke({\"messages\": [(\"human\", \"What topic is most talked about? \")]})\n",
    "print(output[\"messages\"][-1].content)\n",
    "**LangSmith Trace**: https://smith.langchain.com/public/9c8afcce-0ed1-4fb1-b719-767e6432bd8e/r"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
